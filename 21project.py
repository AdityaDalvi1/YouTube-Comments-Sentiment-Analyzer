# -*- coding: utf-8 -*-
"""21project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q7P8qv1iiwPjaC7W6yKU0oZ_I32hGxt3
"""

pip install matplotlib

import matplotlib.pyplot as plt

# Data
algorithms = ['SVC', 'KN', 'NB', 'DT', 'RF', 'BgC', 'ETC', 'AdaBoost', 'LR', 'GBDT', 'xgb']
accuracy = [0.541787, 0.587896, 0.822767, 0.871758, 0.930836, 0.958213, 0.948127, 0.951009, 0.935159, 0.939481, 0.922190]
precision = [0.541787, 0.606635, 0.761905, 0.814004, 0.936170, 0.937028, 0.942708, 0.959677, 0.966197, 0.979885, 0.930481]

# Plot
fig, ax = plt.subplots(figsize=(12, 8))

bar_width = 0.35
index = range(len(algorithms))

bar1 = ax.bar(index, accuracy, bar_width, label='Accuracy')
bar2 = ax.bar([i + bar_width for i in index], precision, bar_width, label='Precision')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Scores')
ax.set_title('Accuracy and Precision of Different Algorithms')
ax.set_xticks([i + bar_width / 2 for i in index])
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Data
algorithms = ['SVC', 'KN', 'NB', 'DT', 'RF', 'BgC', 'ETC', 'AdaBoost', 'LR', 'GBDT', 'xgb']
accuracy = [0.541787, 0.587896, 0.822767, 0.871758, 0.930836, 0.958213, 0.948127, 0.951009, 0.935159, 0.939481, 0.922190]
precision = [0.541787, 0.606635, 0.761905, 0.814004, 0.936170, 0.937028, 0.942708, 0.959677, 0.966197, 0.979885, 0.930481]

# Plot for Accuracy
fig, ax = plt.subplots(figsize=(12, 6))
index = range(len(algorithms))
bar_width = 0.4

bar1 = ax.bar(index, accuracy, bar_width, color='b', label='Accuracy')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Accuracy Score')
ax.set_title('Accuracy of Different Algorithms for Document Frequency')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

# Plot for Precision
fig, ax = plt.subplots(figsize=(12, 6))

bar2 = ax.bar(index, precision, bar_width, color='r', label='Precision')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Precision Score')
ax.set_title('Precision of Different Algorithms for Document Frequency')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Data
algorithms = ['GBDT', 'LR', 'AdaBoost', 'ETC', 'BgC', 'RF', 'XGB', 'DT', 'NB', 'KN', 'SVC']
accuracy = [0.939481, 0.935159, 0.951009, 0.948127, 0.958213, 0.930836, 0.922190, 0.871758, 0.822767, 0.587896, 0.541787]
comments = [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600]  # Example comment counts

# Plot
fig, ax = plt.subplots(figsize=(10, 6))

# Plot each algorithm's accuracy against the number of comments
for algo, acc, com in zip(algorithms, accuracy, comments):
    ax.scatter(com, acc, label=algo)

ax.set_xlabel('Number of Comments')
ax.set_ylabel('Accuracy')
ax.set_title('Performance of Models in Individual Categories for Document Frequency')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data for Document Frequency
algorithms = ['GBDT', 'LR', 'AdaBoost', 'ETC', 'BgC', 'RF', 'xgb', 'DT', 'NB', 'KN', 'SVC']
accuracy = [0.939481, 0.935159, 0.951009, 0.948127, 0.958213, 0.930836, 0.922190, 0.871758, 0.822767, 0.587896, 0.541787]
positive_comments = [50, 30, 40, 60, 70, 80, 90, 100, 110, 120, 130]
negative_comments = [20, 10, 30, 40, 50, 60, 70, 80, 90, 100, 110]
neutral_comments = [30, 20, 50, 40, 60, 70, 80, 90, 80, 100, 120]

# Plot for Document Frequency
fig, ax = plt.subplots(figsize=(12, 6))

bar_width = 0.25
index = np.arange(len(algorithms))

bar1 = ax.bar(index - bar_width, positive_comments, bar_width, color='b', label='Positive')
bar2 = ax.bar(index, negative_comments, bar_width, color='r', label='Negative')
bar3 = ax.bar(index + bar_width, neutral_comments, bar_width, color='g', label='Neutral')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Number of Comments')
ax.set_title('performace of all model in individual cataogy for Document Frequency)')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

# Data for TF-IDF (just change the variable names if you have different data)
# For the sake of demonstration, let's reuse the same data here

# Plot for TF-IDF
fig, ax = plt.subplots(figsize=(12, 6))

bar1 = ax.bar(index - bar_width, positive_comments, bar_width, color='b', label='Positive')
bar2 = ax.bar(index, negative_comments, bar_width, color='r', label='Negative')
bar3 = ax.bar(index + bar_width, neutral_comments, bar_width, color='g', label='Neutral')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Number of Comments')
ax.set_title('performace of all model in individual cataogy for TF-IDF')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data for Document Frequency
algorithms = ['GBDT', 'LR', 'AdaBoost', 'ETC', 'BgC', 'RF', 'xgb', 'DT', 'NB', 'KN', 'SVC']
accuracy = [0.939481, 0.935159, 0.951009, 0.948127, 0.958213, 0.930836, 0.922190, 0.871758, 0.822767, 0.587896, 0.541787]
positive_comments = [50, 30, 40, 60, 70, 80, 90, 100, 110, 120, 130]
negative_comments = [20, 10, 30, 40, 50, 60, 70, 80, 90, 100, 110]
neutral_comments = [30, 20, 50, 40, 60, 70, 80, 90, 80, 100, 120]

# Sorting algorithms based on accuracy
sorted_indices = np.argsort(accuracy)[::-1]  # Descending order
algorithms_sorted = [algorithms[i] for i in sorted_indices]

# Sorting comment data accordingly
positive_comments_sorted = [positive_comments[i] for i in sorted_indices]
negative_comments_sorted = [negative_comments[i] for i in sorted_indices]
neutral_comments_sorted = [neutral_comments[i] for i in sorted_indices]

# Plot
fig, ax = plt.subplots(figsize=(12, 6))

bar_width = 0.25
index = np.arange(len(algorithms_sorted))

bar1 = ax.bar(index - bar_width, positive_comments_sorted, bar_width, color='b', label='Positive')
bar2 = ax.bar(index, negative_comments_sorted, bar_width, color='r', label='Negative')
bar3 = ax.bar(index + bar_width, neutral_comments_sorted, bar_width, color='g', label='Neutral')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Number of Comments')
ax.set_title('Number of Comments by Category for Different Algorithms (Sorted by Accuracy)')
ax.set_xticks(index)
ax.set_xticklabels(algorithms_sorted, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define your testing accuracy data for each model at different data sizes
# Here, I'll provide dummy data as placeholders
models = ['BgC', 'AdaBoost', 'ETC', 'GBDT', 'LR', 'RF', 'xgb']
data_sizes = range(100, 5100, 100)  # Assuming data size ranges from 100 to 5000 comments

# Dummy testing accuracy data for demonstration purposes
testing_accuracy = {
    'BgC': [0.958, 0.948, 0.939, 0.932, 0.925, 0.920, 0.915, 0.910, 0.906, 0.902, 0.898, 0.895, 0.892, 0.889, 0.887, 0.884, 0.882, 0.880, 0.878, 0.876, 0.874, 0.873, 0.871, 0.869, 0.868, 0.867, 0.865, 0.864, 0.863, 0.862, 0.861, 0.860, 0.859, 0.858, 0.857, 0.856, 0.855, 0.854, 0.853, 0.852, 0.851, 0.850, 0.849, 0.848, 0.847, 0.846, 0.845, 0.845, 0.844, 0.843],
    'AdaBoost': [0.951, 0.941, 0.932, 0.926, 0.921, 0.917, 0.913, 0.909, 0.906, 0.903, 0.900, 0.897, 0.895, 0.893, 0.891, 0.889, 0.887, 0.885, 0.884, 0.882, 0.881, 0.880, 0.878, 0.877, 0.876, 0.875, 0.874, 0.873, 0.872, 0.871, 0.870, 0.869, 0.868, 0.867, 0.866, 0.865, 0.864, 0.864, 0.863, 0.862, 0.861, 0.861, 0.860, 0.859, 0.859, 0.858, 0.857, 0.857, 0.856, 0.856],
    'ETC': [0.948, 0.938, 0.928, 0.920, 0.914, 0.909, 0.905, 0.901, 0.898, 0.895, 0.892, 0.889, 0.887, 0.884, 0.882, 0.880, 0.878, 0.876, 0.875, 0.873, 0.872, 0.870, 0.869, 0.868, 0.867, 0.865, 0.864, 0.863, 0.862, 0.861, 0.860, 0.859, 0.858, 0.857, 0.856, 0.855, 0.854, 0.853, 0.852, 0.851, 0.850, 0.849, 0.848, 0.847, 0.847, 0.846, 0.845, 0.844, 0.843, 0.842],
    'GBDT': [0.939, 0.925, 0.911, 0.901, 0.895, 0.889, 0.884, 0.879, 0.875, 0.871, 0.867, 0.863, 0.860, 0.857, 0.854, 0.851, 0.848, 0.846, 0.844, 0.841, 0.839, 0.837, 0.835, 0.833, 0.831, 0.829, 0.827, 0.825, 0.824, 0.822, 0.821, 0.819, 0.818, 0.816, 0.815, 0.813, 0.812, 0.810, 0.809, 0.808, 0.807, 0.805, 0.804, 0.803, 0.802, 0.801, 0.800, 0.799, 0.798, 0.797, 0.796, 0.795],
    'LR': [0.935, 0.921, 0.907, 0.898, 0.891, 0.886, 0.882, 0.879, 0.876, 0.873, 0.870, 0.867, 0.865, 0.863, 0.861, 0.859, 0.857, 0.856, 0.854, 0.853, 0.852, 0.851, 0.849, 0.848, 0.847, 0.846, 0.845, 0.844, 0.843, 0.842, 0.841, 0.840, 0.840, 0.839, 0.838, 0.837, 0.837, 0.836, 0.835, 0.835, 0.834, 0.833, 0.833, 0.832, 0.832, 0.831, 0.831, 0.830, 0.829, 0.829],
    'RF': [0.931, 0.916, 0.901, 0.890, 0.880, 0.872, 0.865, 0.858, 0.852, 0.846, 0.841, 0.837, 0.832, 0.828, 0.824, 0.821, 0.818, 0.815, 0.812, 0.809, 0.807, 0.804, 0.802, 0.800, 0.797, 0.795, 0.793, 0.791, 0.789, 0.787, 0.785, 0.783, 0.782, 0.780, 0.779, 0.777, 0.776, 0.774, 0.773, 0.772, 0.770, 0.769, 0.768, 0.767, 0.766, 0.765, 0.764, 0.763, 0.762, 0.761],
    'xgb': [0.922, 0.907, 0.892, 0.881, 0.872, 0.864, 0.857, 0.851, 0.845, 0.840, 0.836, 0.832, 0.828, 0.824, 0.821, 0.818, 0.815, 0.812, 0.810, 0.807, 0.805, 0.803, 0.801, 0.799, 0.797, 0.795, 0.793, 0.792, 0.790, 0.788, 0.787, 0.785, 0.784, 0.782, 0.781, 0.780, 0.779, 0.777, 0.776, 0.775, 0.774, 0.773, 0.772, 0.771, 0.770, 0.769, 0.768, 0.768, 0.767, 0.766]
}

# Plotting the graph
plt.figure(figsize=(10, 6))

for model in models:
    plt.plot(data_sizes, testing_accuracy[model][:len(data_sizes)], label=model)

plt.title('Testing Accuracy vs. Data Size')
plt.xlabel('Data Size (Number of Comments)')
plt.ylabel('Testing Accuracy')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Define your testing accuracy data for each model at different data sizes
# Here, I'll provide dummy data as placeholders
models = ['BgC', 'AdaBoost', 'ETC', 'GBDT', 'LR', 'RF', 'xgb']
data_sizes = range(100, 5100, 100)  # Assuming data size ranges from 100 to 5000 comments

# Dummy testing accuracy data for demonstration purposes
testing_accuracy = {
    'BgC': [0.958, 0.948, 0.939, 0.932, 0.925, 0.920, 0.915, 0.910, 0.906, 0.902, 0.898, 0.895, 0.892, 0.889, 0.887, 0.884, 0.882, 0.880, 0.878, 0.876, 0.874, 0.873, 0.871, 0.869, 0.868, 0.867, 0.865, 0.864, 0.863, 0.862, 0.861, 0.860, 0.859, 0.858, 0.857, 0.856, 0.855, 0.854, 0.853, 0.852, 0.851, 0.850, 0.849, 0.848, 0.847, 0.846, 0.845, 0.845, 0.844, 0.843],
    'AdaBoost': [0.951, 0.941, 0.932, 0.926, 0.921, 0.917, 0.913, 0.909, 0.906, 0.903, 0.900, 0.897, 0.895, 0.893, 0.891, 0.889, 0.887, 0.885, 0.884, 0.882, 0.881, 0.880, 0.878, 0.877, 0.876, 0.875, 0.874, 0.873, 0.872, 0.871, 0.870, 0.869, 0.868, 0.867, 0.866, 0.865, 0.864, 0.864, 0.863, 0.862, 0.861, 0.861, 0.860, 0.859, 0.859, 0.858, 0.857, 0.857, 0.856, 0.856],
    'ETC': [0.948, 0.938, 0.928, 0.920, 0.914, 0.909, 0.905, 0.901, 0.898, 0.895, 0.892, 0.889, 0.887, 0.884, 0.882, 0.880, 0.878, 0.876, 0.875, 0.873, 0.872, 0.870, 0.869, 0.868, 0.867, 0.865, 0.864, 0.863, 0.862, 0.861, 0.860, 0.859, 0.858, 0.857, 0.856, 0.855, 0.854, 0.853, 0.852, 0.851, 0.850, 0.849, 0.848, 0.847, 0.847, 0.846, 0.845, 0.844, 0.843, 0.842],
    'GBDT': [0.939, 0.925, 0.911, 0.901, 0.895, 0.889, 0.884, 0.879, 0.875, 0.871, 0.867, 0.863, 0.860, 0.857, 0.854, 0.851, 0.848, 0.846, 0.844, 0.841, 0.839, 0.837, 0.835, 0.833, 0.831, 0.829, 0.827, 0.825, 0.824, 0.822, 0.821, 0.819, 0.818, 0.816, 0.815, 0.813, 0.812, 0.810, 0.809, 0.808, 0.807, 0.805, 0.804, 0.803, 0.802, 0.801, 0.800, 0.799, 0.798, 0.797, 0.796, 0.795],
    'LR': [0.935, 0.921, 0.907, 0.898, 0.891, 0.886, 0.882, 0.879, 0.876, 0.873, 0.870, 0.867, 0.865, 0.863, 0.861, 0.859, 0.857, 0.856, 0.854, 0.853, 0.852, 0.851, 0.849, 0.848, 0.847, 0.846, 0.845, 0.844, 0.843, 0.842, 0.841, 0.840, 0.840, 0.839, 0.838, 0.837, 0.837, 0.836, 0.835, 0.835, 0.834, 0.833, 0.833, 0.832, 0.832, 0.831, 0.831, 0.830, 0.829, 0.829],
    'RF': [0.931, 0.916, 0.901, 0.890, 0.880, 0.872, 0.865, 0.858, 0.852, 0.846, 0.841, 0.837, 0.832, 0.828, 0.824, 0.821, 0.818, 0.815, 0.812, 0.809, 0.807, 0.804, 0.802, 0.800, 0.797, 0.795, 0.793, 0.791, 0.789, 0.787, 0.785, 0.783, 0.782, 0.780, 0.779, 0.777, 0.776, 0.774, 0.773, 0.772, 0.770, 0.769, 0.768, 0.767, 0.766, 0.765, 0.764, 0.763, 0.762, 0.761],
    'xgb': [0.922, 0.907, 0.892, 0.881, 0.872, 0.864, 0.857, 0.851, 0.845, 0.840, 0.836, 0.832, 0.828, 0.824, 0.821, 0.818, 0.815, 0.812, 0.810, 0.807, 0.805, 0.803, 0.801, 0.799, 0.797, 0.795, 0.793, 0.792, 0.790, 0.788, 0.787, 0.785, 0.784, 0.782, 0.781, 0.780, 0.779, 0.777, 0.776, 0.775, 0.774, 0.773, 0.772, 0.771, 0.770, 0.769, 0.768, 0.768, 0.767, 0.766]
}

# Plotting the graph
plt.figure(figsize=(10, 6))

for model in models:
    plt.plot(data_sizes, testing_accuracy[model][:len(data_sizes)], label=model)

plt.title('Testing Accuracy vs. Data Size (TF-IDF)')
plt.xlabel('Data Size (Number of Comments)')
plt.ylabel('Testing Accuracy')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data for Document Frequency
algorithms = ['BgC', 'AdaBoost', 'ETC', 'GBDT', 'LR', 'RF', 'xgb']
accuracy = [0.958213, 0.951009, 0.948127, 0.939481, 0.935159, 0.930836, 0.922190]
positive_comments = [100, 40, 60, 50, 30, 80, 90]
negative_comments = [70, 30, 40, 20, 10, 60, 70]
neutral_comments = [80, 50, 40, 30, 20, 70, 80]

# Plot for Document Frequency
fig, ax = plt.subplots(figsize=(12, 6))

bar_width = 0.25
index = np.arange(len(algorithms))

bar1 = ax.bar(index - bar_width, positive_comments, bar_width, color='b', label='Positive')
bar2 = ax.bar(index, negative_comments, bar_width, color='r', label='Negative')
bar3 = ax.bar(index + bar_width, neutral_comments, bar_width, color='g', label='Neutral')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Number of Comments')
ax.set_title('Performance of Algorithms in Individual Categories (Document Frequency)')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data for TF-IDF
algorithms = ['BgC', 'AdaBoost', 'ETC', 'GBDT', 'LR', 'RF', 'xgb']
accuracy = [0.958213, 0.951009, 0.948127, 0.939481, 0.935159, 0.930836, 0.922190]
positive_comments = [100, 40, 60, 50, 30, 80, 90]
negative_comments = [70, 30, 40, 20, 10, 60, 70]
neutral_comments = [80, 50, 40, 30, 20, 70, 80]

# Plot for TF-IDF
fig, ax = plt.subplots(figsize=(12, 6))

bar_width = 0.25
index = np.arange(len(algorithms))

bar1 = ax.bar(index - bar_width, positive_comments, bar_width, color='b', label='Positive')
bar2 = ax.bar(index, negative_comments, bar_width, color='r', label='Negative')
bar3 = ax.bar(index + bar_width, neutral_comments, bar_width, color='g', label='Neutral')

ax.set_xlabel('Algorithms')
ax.set_ylabel('Number of Comments')
ax.set_title('Performance of Algorithms in Individual Categories (TF-IDF)')
ax.set_xticks(index)
ax.set_xticklabels(algorithms, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Hypothetical data
data_sizes = [100, 200, 300, 400, 500, 600]  # Example data sizes

# Example testing accuracy for each model
models = ['LR', 'RF', 'AdaBoost', 'BgC', 'ETC', 'GBDT', 'xgb']
accuracy_data = {
    'LR': [0.85, 0.88, 0.91, 0.92, 0.94, 0.95],
    'RF': [0.82, 0.84, 0.86, 0.88, 0.90, 0.92],
    'AdaBoost': [0.86, 0.88, 0.90, 0.92, 0.93, 0.94],
    'BgC': [0.90, 0.91, 0.92, 0.93, 0.94, 0.95],
    'ETC': [0.83, 0.85, 0.87, 0.89, 0.91, 0.93],
    'GBDT': [0.88, 0.89, 0.90, 0.91, 0.92, 0.93],
    'xgb': [0.80, 0.82, 0.85, 0.87, 0.89, 0.91]
}

# Plotting the graph for each model
plt.figure(figsize=(10, 6))
for model in models:
    plt.plot(data_sizes, accuracy_data[model], marker='o', label=model)

# Adding labels and title
plt.xlabel('Data Size')
plt.ylabel('Testing Accuracy')
plt.title('Testing Accuracy vs. Data Size plot for seven different Models (df)')

# Display the graph
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Hypothetical data
data_sizes = [100, 200, 300, 400, 500, 600]  # Example data sizes

# Example testing accuracy for each model
models = ['LR', 'RF', 'AdaBoost', 'BgC', 'ETC', 'GBDT', 'xgb']
accuracy_data = {
    'LR': [0.85, 0.88, 0.91, 0.92, 0.94, 0.95],
    'RF': [0.82, 0.84, 0.86, 0.88, 0.90, 0.92],
    'AdaBoost': [0.86, 0.88, 0.90, 0.92, 0.93, 0.94],
    'BgC': [0.90, 0.91, 0.92, 0.93, 0.94, 0.95],
    'ETC': [0.83, 0.85, 0.87, 0.89, 0.91, 0.93],
    'GBDT': [0.88, 0.89, 0.90, 0.91, 0.92, 0.93],
    'xgb': [0.80, 0.82, 0.85, 0.87, 0.89, 0.91]
}

# Plotting the graph for each model
plt.figure(figsize=(10, 6))
for model in models:
    plt.plot(data_sizes, accuracy_data[model], marker='o', label=model)

# Adding labels and title
plt.xlabel('Data Size')
plt.ylabel('Testing Accuracy')
plt.title('Testing Accuracy vs. Data Size plot for seven different Models (tf -idf)')

# Setting y-axis ticks
plt.yticks([0.8, 0.9, 1.0])

# Display the graph
plt.legend()
plt.grid(True)
plt.show()

# Assign algorithm names and accuracies from the provided table
algorithm_accuracies = {
    "XGBoost": 0.9304,
    "Random Forest": 0.9308,
    "Logistic Regression": 0.9351,
    "Gradient Boosting Decision Trees": 0.9394,
    "Extra Trees Classifier": 0.9481,
    "AdaBoost": 0.951,
    "Bagging Classifier": 0.9582
}

# Sentiment analysis accuracy from the project
sentiment_analysis_accuracy = 0.95
# Compare sentiment analysis accuracy with algorithm accuracies
comparison_result = {}
for algorithm, acc in algorithm_accuracies.items():
    comparison_result[algorithm] = "Higher" if sentiment_analysis_accuracy > acc else "Lower"

# Print comparison result
print("Comparison of Sentiment Analysis Accuracy with Algorithm Accuracies:")
for algorithm, acc in algorithm_accuracies.items():
    print(f"{algorithm}: Project Accuracy is {comparison_result[algorithm]} than {acc}")

# Discussion
print("\nDiscussion:")
print("The sentiment analysis project achieved an accuracy of", sentiment_analysis_accuracy)
print("The project accuracy is relatively", comparison_result)
print("This indicates that the sentiment analysis project performed comparably well against the machine learning algorithms listed in the table.")
print("However, it's important to note that direct comparison may not be entirely fair due to differences in dataset, feature engineering, and model complexity.")
print("Further analysis, including cross-validation and testing on diverse datasets, would provide a more comprehensive evaluation.")